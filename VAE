import tensorflow as tf
from tensorflow.keras import layers
import numpy as np
import matplotlib.pyplot as plt

# -------------------------
# Load MNIST and preprocess
# -------------------------
(x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()
x_train = x_train[:10000].astype('float32') / 127.5 - 1  # scale to [-1,1]
x_train = np.expand_dims(x_train, axis=-1)

BATCH_SIZE = 64
LATENT_DIM = 5   # slightly larger for better generation
EPOCHS = 5

# Dataset yields (x, x) for reconstruction
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, x_train)) \
                               .shuffle(10000).batch(BATCH_SIZE)

# -------------------------
# Sampling layer
# -------------------------
class Sampling(layers.Layer):
    def call(self, inputs):
        z_mean, z_log_var = inputs
        batch = tf.shape(z_mean)[0]
        dim = tf.shape(z_mean)[1]
        epsilon = tf.random.normal(shape=(batch, dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon

# -------------------------
# Encoder
# -------------------------
encoder_inputs = layers.Input(shape=(28,28,1))
# Encoder
x = layers.Conv2D(64, 3, activation='relu', strides=2, padding='same')(encoder_inputs)
x = layers.Conv2D(128, 3, activation='relu', strides=2, padding='same')(x)
x = layers.Flatten()(x)
x = layers.Dense(64, activation='relu')(x)
z_mean = layers.Dense(LATENT_DIM)(x)
z_log_var = layers.Dense(LATENT_DIM)(x)
z = Sampling()([z_mean, z_log_var])
encoder = tf.keras.Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')

# -------------------------
# Decoder
# -------------------------
latent_inputs = layers.Input(shape=(LATENT_DIM,))
x = layers.Dense(7*7*128, activation='relu')(latent_inputs)
x = layers.Reshape((7,7,128))(x)
x = layers.Conv2DTranspose(128, 3, strides=2, padding='same', activation='relu')(x)
x = layers.Conv2DTranspose(64, 3, strides=2, padding='same', activation='relu')(x)
decoder_outputs = layers.Conv2DTranspose(1, 3, padding='same', activation='tanh')(x)
decoder = tf.keras.Model(latent_inputs, decoder_outputs, name='decoder')

# -------------------------
# VAE Model
# -------------------------
class VAE(tf.keras.Model):
    def __init__(self, encoder, decoder):
        super(VAE, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
    def call(self, x):
        z_mean, z_log_var, z = self.encoder(x)
        reconstructed = self.decoder(z)
        # KL divergence loss
        kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)
        self.add_loss(kl_loss)
        return reconstructed

vae = VAE(encoder, decoder)
vae.compile(optimizer='adam', loss='mse')

# -------------------------
# Training with inline visualization
# -------------------------
for epoch in range(EPOCHS):
    history = vae.fit(train_dataset, epochs=1, verbose=0)
    print(f'Epoch {epoch+1}, Loss={history.history["loss"][0]:.4f}')
    
    # Visualize reconstructions
    sample = x_train[:16]
    reconstructed = vae(sample).numpy()
    fig, axes = plt.subplots(2,16, figsize=(16,2))
    for i in range(16):
        axes[0,i].imshow(sample[i,:,:,0], cmap='gray')
        axes[0,i].axis('off')
        axes[1,i].imshow(reconstructed[i,:,:,0], cmap='gray')
        axes[1,i].axis('off')
    plt.suptitle(f'Reconstructions at Epoch {epoch+1}')
    plt.show()

# -------------------------
# 2D Latent space visualization (first 2 dimensions)
# -------------------------
z_mean, z_log_var, z = encoder(x_train[:5000])
plt.figure(figsize=(6,6))
plt.scatter(z[:,0], z[:,1], alpha=0.5)
plt.title('2D Latent Space (first 2 dimensions)')
plt.xlabel('z[0]')
plt.ylabel('z[1]')
plt.show()
